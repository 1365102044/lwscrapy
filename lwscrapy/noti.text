
**********************************创建scrapy项目：**********************************
scrapy startproject projectname

**********************************结构：**********************************
1、spiders文件夹：爬虫文件主目录
2、init.py：将改文件夹变为一个python模块
3、items.py：定义所需要爬虫的项目
4、middlewares.py：爬虫中间件
5、pipelines.py：管道文件
6、settings.py：设置文件


用scrapy.Spider类创建一个子类来建立一个Spider，其下有三个强制的属性 和 一个方法。

name = “”
爬虫的唯一识别名称

allow_domains = []
搜索的域名范围
规定爬虫只爬取这个域名下的网页，不存在的URL会被忽略。

start_urls = ()
爬取起始URL
爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些urls开始。其他子URL将会从这些元祖URL中继承性生成。

parse(self, response)
解析的方法
URL传回Response对象作为唯一参数，用来

解析返回的网页数据(response.body)
提取结构化数据(生成item)
生成需要下一页的URL请求。



**********************************运行方式：**********************************
scrapy crawl name(自定义的识别名称)



**********************************保存数据**********************************

-o 输出指定格式的文件，命令如下：

json 格式
scrapy crawl Medicaldata -o drug.json

json lines格式，默认为Unicode编码
scrapy crawl Medicaldata -o drug.json

csv 逗号表达式，可用Excel打开
scrapy crawl Medicaldata -o drug.csv

xml格式
scrapy crawl Medicaldata -o drug.xml


解决方案
在settings.py文件中增加一行，导出时强制为’utf-8’即可转换为中文
FEED_EXPORT_ENCODING = 'utf-8'



**********************************log配置：**********************************

Log levels
Scrapy提供5层logging级别:
CRITICAL - 严重错误(critical)
ERROR - 一般错误(regular errors)
WARNING - 警告信息(warning messages)
INFO - 一般信息(informational messages)
DEBUG - 调试信息(debugging messages)

LOG_ENABLED 默认: True，启用logging
LOG_ENCODING 默认: 'utf-8'，logging使用的编码
LOG_FILE 默认: None，在当前目录里创建logging输出文件的文件名
LOG_LEVEL 默认: 'DEBUG'，log的最低级别
LOG_STDOUT 默认: False 如果为 True，进程所有的标准输出(及错误)将会被重定向到log中。例如，执行 print "hello" ，其将会在Scrapy log中显示。



**********************************自定义图片下载管道中间件，图片下载**********************************

1、自己在pipelines文件中 重写图片下载管道中间件，
   并在setting文件中 配置自定义的中间件路径以及图片下载的途径、图片url对应item中的url字段
2、注意点：图片url在item对应的应该是数组形式
   不然报错:ValueError: Missing scheme in request url: h,
   原因是 中间件中获取不到正确的 图片url



**********************************Itemloader**********************************

ItemLoader是负责数据的收集、处理、填充,item仅仅是承载了数据本身
数据的收集、处理、填充归功于item loader中两个重要组件:
    -输入处理input processors
    -输出处理output processors

ItemLoader 类位于 scrapy.loader ，它可以接收一个 Item 实例来指定要加载的 Item, 然后指定 response 或者 selector 来确定要解析的内容，
最后提供了 add_css()、 add_xpath() 方法来对通过 css 、 xpath 解析赋值，还有 add_value() 方法来单独进行赋值。
可以看到无论解析出来的值的数量是多少，ItemLoader 默认都会返回一个 list。在之前的方式中我们都是通过 extract_first() 获取第一个值或者通过 extract() 解析到值后进行遍历的。
在 ItemLoader 中，为我们提供了 processor 来对数据进行处理。

在 ItemLoader 类中，提供了 default_output_processor 和 default_input_processor 来对数据的输入与输出进行解析，
如果我们需要只获取解析后的第一个值，可以指定 default_output_processor 为 TakeFirst() 即可，这是 Scrapy 提供的一个解析处理类，




**********************************splash的使用：**********************************

1、splash使用的是Splash HTTP API，所以需要一个Splash Instance,一般采用docker运行splash，所以需要安装docker

sudo apt-get install docker

2、拉取镜像

sudo docker pull scrapinghub/splash

3、在本机的8050和8051端口开启Splash服务

docker run -p 8050:8050 -p 8051:8051 scrapinghub/splash
docker run -p 8050:8050 scrapinghub/splash
4、安装scrapy-splash

pip install scrapy-splash

5、配置splash服务（settings.py）

# 配置splash

```
# Splash服务器地址
SPLASH_URL = 'http://localhost:8050'
DOWNLOADER_MIDDLEWARES = {
'scrapy_splash.SplashCookiesMiddleware': 723,
'scrapy_splash.SplashMiddleware': 725,
'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
}

SPIDER_MIDDLEWARES = {
'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,
}
# 设置去重过滤器
DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'
# 用来支持cache_args
HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'

```



**********************************在管道中判断 spider：**********************************

1、通过spider
    if spider.name == 'SpiderXX':
2.通过item类型
    if isinstance(item, AAItem):


**********************************

使用setting文件中的配置数据

self.setting.get('keyname')


**********************************

创建spider文件（在 spider 文件路径下，）

scarpy genspider 文件名 ‘url’


# 重写 自定义开始请求

start_requests
自定义URL

post 请求使用： scrapy.FormRequest()




**********************************Downloader Middlewares (下载器中间件)**********************************

下载器中间件是引擎和下戟器之间通信的中间件。在这个中间件中我们可以设置代理、更换请求头等来达到反反爬虫的目的。要写下
载器中间件，可以在下载器中实现两个方法。-一个是process. request(self,request,spider)，这个方法是在请求发送之前会执行，
还有一个是process response(self,request,response,spider)，这个方法是数据下载到引擎之前执行。
process_ request(self,request,spider):
这个方法是下载器在发送请求之前会执行的。- -般可以在这个里面设置随机代理p等。
1.参数:
    。request: 发送请求的request对象。
    。spider:发送请求的spider对象。
2.返回值:
    。返回None:如果返回None, Scrapy将继续处理该request, 执行其他中间件中的相应方法，直到合适的下载器处理函数被调用。
    。返回Response对象: Scrapy将不会调用任何其他的process request方法，将直接返回这个response对象。已经激活的中
        间件的process_ response0方法则会在每个response返回时被调用。
    。返回Request对象:不再使用之前的request对象去下载数据，而是根据现在返回的request对象返回数据。
    。如果这个方法中抛出了异常,则会调用process_ .exception方法。

process_ response(self,request,response,spider):
这个是下载器下载的数据到引擎中间会执行的方法。
1.参数:
    。request: request对象 。
    。response:被处理的response对象。
    。spider: spider对象 。
2.返回值:
    。返回Response对象: 会将这个新的response对象传给其他中间件，最终传给爬虫。
    。返回Request对象:下载器链被切断,返回的request会重新被下载器调度下载。
    。如果抛出-一个异常,那么调用request的errback方法，如果没有指定这个方法，那么会抛出- -个异常。




httpbin.org/ip
httpbin.org/user-agent



********************************scrapy shell 调试工具************************************

在项目路径下，
$ scrapy shell url
进入调试， 可以先使用 xpath/css/re 进行尝试获取数据，看看是否正确





******************************** CrawlSpider ********************************

我们是自己在解析完整个页面后获取下一页的url,然后重新发送一个请求。 有时候我们想要这样做，只要满足某个条件的url,
都给我进行爬取。那么这时候我们就可以通过CrawlSpider 来帮我们完成了。CrawlSpider 继承自Spider ，
只不过是在之前的基础之上增加了新的功能，可以定义爬取的url的规则，以后scrapy碰到满足条件的ur|都进行爬取，而
不用手动的yield Request。

创建CrawlSpider爬虫:
   之前创建爬虫的方式是通过scrapy genspider [爬虫名字] [城名]的方式创建的。如果想要创建CrawlSpider 爬虫，那么应该通过以
    下命令创建:
    $ scrapy genspider -t crawl [爬虫名字] [域名]
    # 创建普通爬虫文件
    $ scrapy genspider crawl [爬虫名字] [域名]

LinkExtractors链接提取器:
使用LinkExtractors 可以不用程序员自己提取想要的url,然后发送请求。这些工作都可以交给LinkExtractors ,他会在所有爬的页
面中找到满足规则的url，实现自动的爬取。以下对LinkExtractors 类做- -个简单的介绍:

class scrapy.linkextractors.LinkExtractor(
    allow = (),
    deny=()
    allow_ _domains = ()。
    deny_ domains = () ，
    deny_ extensions = None ，
    restrict_ xpaths = ( )，
    tags = ('a','area'),
    attrs = ('href'),
    canonicalize = True ,
    unique = True ,
    process_ value = None
)
主要参数讲解:
    ● allow:允许的url。所有满足这个正则表达式的url都会被提取。
    ● deny: 禁止的url。所有满足这个正则表达式的url都不会被提取。
    ● allow_ domains: 允许的域名。只有在这个里面指定的域名的url才会被提取。
    ● deny_domains:禁止的域名。所有在这个里面指定的域名的url都不会被提取。
    ● restrict_xpaths:严格的xpath。和allow共同过滤链接。


Rule规则类:
定义爬虫的规则类。以下对这个类做- -个简单的介绍:
class scrapy.spiders.Rule(
    link_extractor,
    callback = None,
    cb_kwargs = None ,
    follow = None ,
    process_links = None,
    process_request = None,
)
主要参数讲解:
    ● link extractor:一个LinkExtractor 对象，用于定义爬取规则。
    ● callback:满足这个规则的url,应该要执行哪个回调函数。因为CrawlSpider 使用了parse作为回调函数，因此不要覆
    盖parse作为回调函数自己的回调函数。
    ● follow: 指定根据该规则从response中提取的链接是否需要跟进。
    ● process_ links: 以link extractor中获取到接后会传递给这个函数，用来过滤不需要爬取的链接。


### CrawlSpider:
需要使用~LinkExtractor~和^Rule.这两个东西决定爬虫的具体走向。
1. allow设置规则的方法:要能够限制在我们想要的url上面。不要跟其他的ur1产生相同
的正则表达式即可。
2.什么情况下使用follow: 如果在爬取页面的时候，需要将满足当前条件的url再进行跟
进，那么就设置为True。否则设置为Fasle.
3.什么情况下该指定callback: 如果这个ur1对应的页面， 只是为了获取更多的url,并不
需要里面的数据，那么可以不指定callback。如果想要获取url对应页面中的数据，那么就
需要指定一个callback。


